# ============================================================================
# CodeCrucible Synth - COMPLETE Unified Configuration v2.0
# ============================================================================
# This master configuration file consolidates ALL features from:
# - config/default.yaml
# - codecrucible.config.json  
# - config/unified-model-config.yaml
# - config/hybrid.yaml
# - config/hybrid-config.json
# - config/optimized-model-config.json
# 
# Generated: 2025-08-27
# Following AI Coding Grimoire principles
# ============================================================================

# Configuration metadata (required)
metadata:
  version: "2.0"
  description: "CodeCrucible Synth Complete Unified Configuration"
  created: "2025-08-27"
  last_modified: "2025-08-27"
  precedence_level: 20
  applies_to: ["development", "production", "testing"]
  config_id: "unified-master-complete"

# ============================================================================
# APPLICATION CONFIGURATION
# ============================================================================
app:
  name: "CodeCrucible Synth"
  version: "4.2.4"
  environment: "${NODE_ENV:development}"
  log_level: "${LOG_LEVEL:info}"
  debug_mode: false
  telemetry_enabled: true  
  # Application features
  features:
    enable_voice_system: true
    enable_council_engine: true
    enable_hybrid_models: true
    enable_mcp_integration: true
    enable_streaming: true
    enable_caching: true
    enable_monitoring: true
    max_concurrent_voices: 5
    cache_expiry: 3600

# ============================================================================
# SAFETY & TERMINAL CONFIGURATION (from default.yaml)
# ============================================================================
safety:
  command_validation: true
  file_system_restrictions: false  # Temporarily disabled for testing
  require_consent: ["delete"]       # Removed execute from consent requirements
  
terminal:
  shell: "auto"  # auto-detect or specify: bash, zsh, cmd, powershell
  prompt: "CC> "
  history_size: 1000
  color_output: true# ============================================================================
# VSCODE & AGENT CONFIGURATION
# ============================================================================
vscode:
  auto_activate: true
  inline_generation: true
  show_voice_panel: true

agent:
  enabled: true
  mode: "thorough"  # thorough, fast, auto
  max_concurrency: 1
  enable_caching: false
  enable_metrics: true
  enable_security: true

# ============================================================================
# REPO INDEXER CONFIGURATION
# ============================================================================
indexer:
  include_globs:
    - "**/*.{ts,js,tsx,jsx,py,md,mdx,json,yaml,yml}"
  exclude_globs:
    - "**/node_modules/**"
    - "**/.git/**"
    - "**/dist/**"
    - "**/build/**"
    - "**/.next/**"
    - "**/.cache/**"
    - "**/*.{png,jpg,jpeg,gif,ico,svg,pdf,zip,tar,gz}"
  chunk_size: 1200
  chunk_overlap: 200
  embeddings:
    provider: "${EMBEDDING_PROVIDER:openai}"  # openai | huggingface
    model: "${EMBEDDING_MODEL:text-embedding-3-small}"  # e.g., BAAI/bge-small-en-v1.5
    prefix: "${EMBEDDING_PREFIX:}"  # Optional instruction prefix for instructor models

# ============================================================================
# DEBUG CONFIGURATION (from codecrucible.config.json)
# ============================================================================
debug:
  enabled: false
  verbose: false
  log_level: "info"

# ============================================================================
# COMPREHENSIVE MODEL CONFIGURATION
# ============================================================================
model:
  # Global settings - SINGLE SOURCE OF TRUTH
  endpoint: "${OLLAMA_ENDPOINT:http://localhost:11434}"
  name: "llama3.1:8b"  # Default model with excellent function calling
  defaultModel: "llama3.1:8b"  # Used by config-manager.ts
  timeout: "${OLLAMA_TIMEOUT:120000}"  # 2 minutes for complex operations
  
  # ADAPTIVE SLIDING CONTEXT WINDOW (131K max sliding down based on memory)
  max_context_window: "${MODEL_MAX_CONTEXT_WINDOW:131072}"  # Maximum desired context
  adaptive_context: true                                      # Enable sliding algorithm
  context_tiers: [131072, 96000, 64000, 48000, 32000, 24000, 16000, 12000, 8192]
  memory_threshold: 0.8                                      # Use 80% of available GPU memory
  fallback_context: 8192                                     # Minimum safe context
  
  # Token and response limits
  max_tokens: "${MODEL_MAX_TOKENS:131072}"  # Align with max context
  min_tokens: 8192                              # Minimum for functionality
  temperature: "${MODEL_TEMPERATURE:0.1}"      # Low temp for precise function calls
  streaming_enabled: true
  keep_alive: "15m"  # Keep models loaded to avoid reload overhead
  
  # Provider selection and fallback
  default_provider: "ollama"
  fallback_chain: ["ollama", "lm-studio", "openai", "anthropic", "mock"]
  execution_mode: "auto"  # auto | fast | thorough | custom
  
  # Global timeout settings
  max_retries: 3
  
  # Routing configuration
  routing:
    strategy: "hybrid"  # hybrid, simple, complex
    default_provider: "auto"
    escalation_threshold: 0.6  # From hybrid-config.json
    confidence_scoring: true
    learning_enabled: true    
    # Task complexity mapping
    task_complexity:
      simple: ["format", "template", "boilerplate", "edit", "rename"]
      complex: ["analysis", "architecture", "planning", "debug", "security", "multi-file"]
    
    # Confidence thresholds
    confidence_thresholds:
      high: 0.9
      medium: 0.7
      low: 0.5
      escalation: 0.3
    
    # Performance targets
    performance_targets:
      simple_task_response: 1000
      complex_task_response: 120000  # 2 minutes for complex tasks
      streaming_latency: 100
      fast_mode_threshold: 200  # From optimized-model-config.json
  
  # Model providers configuration
  providers:
    # Ollama configuration (PRIMARY)
    - name: "ollama"
      type: "ollama"
      enabled: true
      priority: 1
      endpoint: "${OLLAMA_ENDPOINT:http://localhost:11434}"      
      # Connection settings (optimized for memory management)
      connection:
        timeout: 5000                 # Quick connection timeout
        response_timeout: 180000       # 3 minutes for complex operations with sliding context
        status_check_timeout: 3000     # Health check timeout
        keep_alive: "15m"              # Keep models loaded
        max_concurrent: 2              # Reduced to prevent memory contention
      
      # MEMORY-AWARE GPU CONFIGURATION
      gpu:
        enabled: true
        adaptive_layers: true          # Enable adaptive GPU layer selection
        max_layers: 33                 # Maximum GPU layers (all layers)
        memory_threshold: 0.8          # Use 80% of available GPU memory
        fallback_layers: [33, 28, 24, 20, 16, 0]  # Sliding GPU layer fallback
        low_vram_mode: false           # Will be enabled automatically if needed
        
      # Performance settings
      performance:
        model_ttl: 1800  # From hybrid-config.json
        keep_alive_time: "15m"
        max_retries: 5
        base_timeout: 120000
        warmup_timeout: 180000
        concurrent_preloads: 2
        
      # Available models with specific settings
      models:
        # PRIORITY-ORDERED MODEL SELECTION (best function calling first)
        preferred:
          - name: "llama3.1:8b"                    # TOP PRIORITY: Best function calling
            temperature: 0.1                       # Precise for function calls
            max_context: 131072                    # Full sliding context support
            memory_requirement: 6.5                # GB memory required
            function_calling: true
            json_mode: true
            optimal_for: ["function_calling", "tools", "analysis", "coding", "debugging"]
          - name: "deepseek-coder:8b"             # Good for code generation
            temperature: 0.15
            max_context: 131072
            memory_requirement: 6.2
            function_calling: true
            optimal_for: ["code_generation", "refactoring", "optimization"]
        fallback:
          - name: "llama3.2:3b"                   # Minimal memory model
            temperature: 0.3
            max_context: 32000
            memory_requirement: 2.8
            function_calling: false
            optimal_for: ["general", "documentation"]
          - name: "gemma:7b"                      # CPU fallback if needed
            temperature: 0.4
            max_context: 32000
            memory_requirement: 1.5                # Can run on CPU
            function_calling: false
            optimal_for: ["general", "simple_tasks"]
      
      # Task routing preferences
      optimal_for: ["analysis", "planning", "complex", "multi-file", "architecture", "debugging"]
      task_types: ["analysis", "planning", "complex", "multi-file"]  # From hybrid.yaml    
    # LM Studio configuration (SECONDARY)
    - name: "lm-studio"
      type: "lmstudio"
      enabled: true
      priority: 2
      endpoint: "${LMSTUDIO_ENDPOINT:http://localhost:1234}"
      
      # Connection settings
      connection:
        timeout: 3000
        response_timeout: 15000
        status_check_timeout: 2000
        max_concurrent: 2  # From hybrid-config.json
        
      # Performance settings from hybrid-config.json
      performance:
        model_ttl: 900
        keep_alive_enabled: true
        keep_alive_interval: 120000
        model_warmup_enabled: true
        connection_timeout: 300000
        request_timeout: 180000
        stream_timeout: 120000  # 2 minutes for streaming
        
      # Available models
      models:
        preferred:
          - name: "codellama-7b-instruct"  # From multiple configs
            temperature: 0.7
            max_tokens: 128000
            context_window: 128000
            optimal_for: ["template", "edit", "format", "boilerplate"]
          - name: "gemma-2b-it"  # From hybrid.yaml
            temperature: 0.5
            max_tokens: 64000
            optimal_for: ["quick_responses", "prototyping"]
          - name: "gemma-3-12b"  # From hybrid-config.json
            temperature: 0.6
            max_tokens: 128000
            optimal_for: ["large-file-processing"]
        fallback:
          - name: "gpt-3.5-turbo"  # OpenAI compatible
            temperature: 0.7
            max_tokens: 8000
            optimal_for: ["general"]
            
      # Task routing
      optimal_for: ["template", "edit", "format", "boilerplate", "streaming", "quick-fix"]
      task_types: ["template", "edit", "format", "boilerplate", "codebase-analysis", "large-file-processing"]
      streaming_enabled: true    
    # OpenAI configuration (from codecrucible.config.json)
    - name: "openai"
      type: "openai"
      enabled: false  # Disabled by default, requires API key
      priority: 3
      api_key: "${OPENAI_API_KEY:}"
      
      models:
        - name: "gpt-4"
          model_id: "gpt-4"
          temperature: 0.7
          max_tokens: 4000
          timeout: 30000
          retries: 3
          optimal_for: ["complex_reasoning", "analysis"]
          
    # Anthropic configuration (from codecrucible.config.json)
    - name: "anthropic"
      type: "anthropic"
      enabled: false  # Disabled by default, requires API key
      priority: 4
      api_key: "${ANTHROPIC_API_KEY:}"
      
      models:
        - name: "claude-3-sonnet"
          model_id: "claude-3-sonnet-20240229"
          temperature: 0.7
          max_tokens: 4000
          timeout: 30000
          retries: 3
          optimal_for: ["writing", "analysis", "reasoning"]

# ============================================================================
# MEMORY-AWARE MODEL PRELOADER CONFIGURATION
# ============================================================================
model_preloader:
  endpoint: "http://localhost:11434"
  
  # Memory-aware model selection
  primary_models:
    - model: "llama3.1:8b"              # Best function calling, load first
      context: 32000                     # Start with safe context for warmup
      memory_requirement: 6.5            # GB
    - model: "deepseek-coder:8b"        # Backup coding model
      context: 32000
      memory_requirement: 6.2
  
  fallback_models:
    - model: "llama3.2:3b"              # Low memory fallback
      context: 16000
      memory_requirement: 2.8
  
  # Preloading behavior
  max_concurrent_loads: 1              # Prevent memory contention
  warmup_prompt: "Test"                # Simple warmup
  keep_alive_time: "15m"               # Keep loaded longer
  retry_attempts: 3                    # More retries for memory issues
  load_timeout: 60000                  # 60s timeout for initial load
  warmup_timeout: 30000                # 30s for warmup
  
  # Memory management during preloading
  check_memory_before_load: true       # Check memory before each load
  auto_unload_on_memory_pressure: true # Unload models if memory low
  memory_check_interval: 30000         # Check memory every 30s# ============================================================================
# COMPREHENSIVE PERFORMANCE CONFIGURATION
# ============================================================================
performance:
  # Global performance settings
  max_concurrent_requests: "${MAX_CONCURRENT_REQUESTS:3}"
  request_timeout: 120000
  response_timeout: 180000
  default_timeout: 60000  # From optimized-model-config.json
  max_timeout: 180000
  
  # Response cache (from default.yaml)
  response_cache:
    enabled: true
    max_age: "${CACHE_TTL:3600000}"  # 1 hour
    max_size: "${RESPONSE_CACHE_SIZE:100}"  # Cache size in MB
    duration: 300000  # From hybrid-config.json
    
  # Voice parallelism (from default.yaml)
  voice_parallelism:
    max_concurrent: "${MAX_CONCURRENT_REQUESTS:3}"
    batch_size: 2
    
  # ADAPTIVE SLIDING CONTEXT MANAGEMENT
  context_management:
    # Sliding window configuration
    strategy: "sliding_adaptive"           # Use sliding adaptive algorithm
    max_context_length: "${MODEL_MAX_CONTEXT_WINDOW:131072}"
    min_context_length: 8192              # Minimum functional context
    sliding_tiers: [131072, 96000, 64000, 48000, 32000, 24000, 16000, 12000, 8192]
    
    # Memory-based adaptation
    memory_threshold: 0.8                 # Use 80% of available GPU memory
    auto_reduce_on_oom: true              # Automatically reduce context on out-of-memory
    context_reduction_factor: 0.75        # Reduce by 25% on memory pressure
    
    # Content management
    compression_threshold: 0.75           # Compress when 75% of context used
    retention_strategy: "hierarchical"    # Keep important content
    chunk_overlap: 1024                   # Reduced overlap for efficiency
    indexing_enabled: true
    
    # Task-specific context allocation (as multipliers of available context)
    task_multipliers:
      analysis: 1.0                       # Full context for analysis
      security: 1.0                       # Full context for security
      architecture: 0.8                   # 80% for architecture
      multi-file: 1.0                     # Full context for multi-file
      debugging: 0.6                      # 60% for debugging
      review: 0.8                         # 80% for reviews
      template: 0.3                       # 30% for templates
      edit: 0.4                           # 40% for edits
      format: 0.3                         # 30% for formatting
      default: 0.5                        # 50% default
      
  # Caching configuration (multiple sources)
  cache:
    enabled: true
    type: "memory"
    ttl: 3600
    max_size: "100MB"
    cache_timeout: 30000  # From optimized-model-config.json
    
  # Resource limits
  memory:
    max_heap: "4096MB"
    warning_threshold: 0.8
    
  # Output configuration and limits
  output:
    max_file_analysis_chars: 8000     # Increased from 2000
    max_search_content_chars: 2000    # Increased from 1000
    max_large_file_preview_chars: 3000 # Increased from 1000
    max_command_output_chars: 50000   # Increased from 30000
    enable_streaming: true            # Enable streaming for large outputs
    truncation_suffix: "... (truncated - use streaming for full content)"
    # Streaming configuration
    streaming_threshold_mb: 10        # Start streaming for files > 10MB
    max_streaming_file_size_mb: 100   # Maximum file size to attempt streaming (100MB)
    streaming_chunk_size: 8192        # Chunk size for streaming reads (8KB)
    max_streaming_lines: 10000        # Maximum lines for readline-based streaming
    
  # MEMORY-AWARE MODEL OPTIMIZATION
  enable_model_caching: true
  enable_fast_mode: false               # Disabled for better memory management
  enable_memory_monitoring: true        # Enable real-time memory tracking
  adaptive_model_selection: true        # Choose models based on available memory
  context_sliding_enabled: true         # Enable sliding context window
  
  # Memory pressure handling
  memory_pressure_thresholds:
    low: 0.6     # < 60% memory usage
    medium: 0.8  # 60-80% memory usage  
    high: 0.9    # 80-90% memory usage
    critical: 0.95 # > 95% memory usage
  
  # Auto-optimization based on memory pressure
  auto_reduce_context_on_pressure: true
  auto_reduce_batch_size_on_pressure: true
  auto_unload_unused_models: true
  memory_cleanup_interval: 60000        # Clean up every 60s
    
  # Monitoring and metrics (from codecrucible.config.json)
  enable_monitoring: true
  alert_threshold: 5000
  metrics_retention: 86400000
  enable_tracing: true
  metrics_port: 9090
  
  # Optimization flags
  optimization:
    lazy_loading: true
    resource_pooling: true
    connection_pooling: true
    auto_optimization: true  # From hybrid.yaml
    vram_optimization: true# ============================================================================
# VOICE SYSTEM CONFIGURATION (from default.yaml)
# ============================================================================
voices:
  enabled: true
  default: ["explorer", "maintainer"]
  available: ["explorer", "maintainer", "analyzer", "developer", "implementor", "security", "architect", "designer", "optimizer"]
  parallel: true
  max_concurrent: 5  # From codecrucible.config.json
  max_concurrent_voices: 5
  consensus_threshold: 0.7
  escalation_threshold: 0.6
  
  # Voice archetypes
  archetypes:
    - name: "explorer"
      enabled: true
      weight: 1.2
      specialization: ["innovation", "creativity", "prototyping"]
    - name: "maintainer"
      enabled: true
      weight: 1.0
      specialization: ["stability", "quality", "testing"]
    - name: "security"
      enabled: true
      weight: 1.1
      specialization: ["vulnerability", "authentication", "compliance"]
    - name: "architect"
      enabled: true
      weight: 1.3
      specialization: ["design", "patterns", "scalability"]
    - name: "developer"
      enabled: true
      weight: 1.0
      specialization: ["implementation", "debugging", "optimization"]

# ============================================================================
# COMPREHENSIVE SECURITY CONFIGURATION
# ============================================================================
security:
  # Authentication and authorization
  authentication:
    enabled: true
    type: "jwt"
    jwt_secret: "${JWT_SECRET:change-in-production}"
    token_expiration: "24h"
    refresh_enabled: true    
  # Security validation (from codecrucible.config.json)
  enable_validation: true
  sandbox_mode: true
  allow_unsafe_commands: false
  enable_rate_limit: true
  max_requests_per_minute: 60
  enable_cors: true
  cors_origins: ["http://localhost:3000"]
  audit_logging: true
  
  # Input validation and security patterns (from unified-model-config.yaml)
  validate_inputs: true
  sanitize_outputs: true
  max_prompt_length: 128000
  max_response_length: 128000
  blocked_patterns: ["password", "secret", "api_key", "private_key"]
  
  # Security policies
  policies:
    max_input_length: 100000
    rate_limiting:
      enabled: true
      requests_per_minute: 60
      requests_per_hour: 1000
    blocked_commands: ["rm -rf", "format", "del /f", "shutdown"]
    allowed_directories: ["./src", "./tests", "./config"]

# ============================================================================
# E2B CODE INTERPRETER CONFIGURATION (from default.yaml)
# ============================================================================
e2b:
  api_key: "${E2B_API_KEY}"
  enabled: true
  enforce_only: true  # Only allow E2B execution, block unsafe local execution
  default_environment: "base"
  session_timeout: 3600000  # 1 hour
  max_concurrent_sessions: 10
  require_authentication: true  # ✅ SECURITY: Authentication required
  strict_mode: true
  timeout: 30000
  max_file_size: "10MB"
  allowed_languages: ["python", "javascript", "typescript", "bash"]
  block_network_access: true
  audit_log: true
  validate_code: true
  block_unsafe_patterns: true
  
  # Resource limits
  resource_limits:
    memory: "512MB"
    cpu: "0.5"
    disk_space: "1GB"
    execution_timeout: 30000
    
  # Security policy
  security:
    strict_mode: true
    allow_network_access: false
    allow_file_system_write: true
    allow_process_spawning: false
    validate_code: true
    audit_log: true
    block_unsafe_patterns: true
    require_authentication: true# ============================================================================
# COMPREHENSIVE MCP (Model Context Protocol) CONFIGURATION
# ============================================================================
mcp:
  enabled: true
  smithery_api_key: "${SMITHERY_API_KEY:}"
  
  # Core MCP servers (from default.yaml)
  servers:
    filesystem:
      enabled: true
      restricted_paths: ["/etc", "/sys", "/proc"]
      allowed_paths: ["~/", "./"]
      allowed_operations: ["read", "write", "list"]
    
    git:
      enabled: true
      auto_commit_messages: false
      safe_mode_enabled: true
      auto_commit: false
    
    terminal:
      enabled: true
      allowed_commands: ["ls", "cat", "grep", "find", "git", "npm", "node", "python"]
      blocked_commands: ["rm -rf", "sudo", "su", "chmod +x"]
      whitelisted_commands: ["npm", "git", "node", "python"]
    
    package_manager:
      enabled: true
      auto_install: false
      security_scan: true
      
  # Smithery AI Configuration (optional)
  smithery:
    enabled: false
    api_key: ""
    profile: ""
    base_url: "https://server.smithery.ai"
  
  # External MCP Servers Configuration (from default.yaml)
  external:
    enabled: true
    servers:
      terminal_controller:
        enabled: true
        api_key: "${MCP_API_KEY}"
        url: "https://server.smithery.ai/@GongRzhe/terminal-controller-mcp/mcp"
      task_manager:
        enabled: true
        api_key: "${MCP_API_KEY}"
        url: "https://server.smithery.ai/@kazuph/mcp-taskmanager/mcp"
      remote_shell:
        enabled: false  # Disabled by default for security
        api_key: "${MCP_API_KEY}"
        url: "https://server.smithery.ai/@samihalawa/remote-shell-terminal-mcp/mcp"
    
    # MCP Security settings (relaxed for testing)
    security:
      validate_commands: true
      allow_remote_execution: false
      require_user_approval: false      # Disabled for testing tool execution

# ============================================================================
# HYBRID SYSTEM CONFIGURATION (from hybrid configs)
# ============================================================================
hybrid:
  enabled: true
  routing_strategy: "intelligent"
  escalation_threshold: 0.6  # Lower value from hybrid-config.json
  confidence_scoring: true
  learning_enabled: true
  
  # Circuit breaker and fallback (from hybrid-config.json)
  fallback:
    auto_fallback: true
    retry_attempts: 3
    retry_delay: 2000
    circuit_breaker:
      enabled: true
      failure_threshold: 5
      recovery_timeout: 60000
  
  # Performance optimization (from hybrid.yaml)
  performance:
    cache_enabled: true
    metrics_collection: true
    auto_optimization: true
    vram_optimization: true
    timeout_strategy: "adaptive"
    circuit_breaker_enabled: true

# ============================================================================
# LOGGING CONFIGURATION (from default.yaml)
# ============================================================================
logging:
  level: "info"
  to_file: true
  max_file_size: "10MB"
  max_files: 5
  format: "json"
  
  # Output destinations
  outputs:
    - type: "console"
      level: "info"
      colorized: true
    - type: "file"
      path: "./logs/codecrucible.log"
      level: "debug"

# ============================================================================
# MONITORING & TELEMETRY (comprehensive)
# ============================================================================
monitoring:
  enabled: true
  
  # Metrics collection (from unified-model-config.yaml)
  metrics:
    collect_performance: true
    collect_errors: true
    collect_usage: true
    port: 3001
    
  # Alert thresholds (from unified-model-config.yaml)
  alerts:
    error_rate: 0.05
    response_time_p99: 5000
    availability: 0.95
    
  # Reporting
  reporting:
    interval: 60000
    endpoint: "${TELEMETRY_ENDPOINT:}"

# ============================================================================
# EXPERIMENTAL FEATURES (from unified-model-config.yaml)
# ============================================================================
experimental:
  dual_agent_review: true
  auto_retry_on_timeout: true
  adaptive_timeouts: true
  connection_pooling: true
  circuit_breaker: false  # Not yet implemented

# ============================================================================
# END OF COMPLETE UNIFIED CONFIGURATION
# ============================================================================
