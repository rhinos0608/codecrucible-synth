/**
 * Optimized Model Client for Enhanced Ollama Performance
 * 
 * Features:
 * - Context reuse and keep-alive connections
 * - Batch processing for multiple requests
 * - Response caching with TTL
 * - Connection pooling and retry logic
 * - Model warm-up and preloading
 * - Memory-efficient context management
 */

import axios, { AxiosInstance } from 'axios';
import { logger } from './logger.js';
import { LocalModelConfig, VoiceResponse, VoiceArchetype, ProjectContext } from './local-model-client.js';

interface CacheEntry {
  response: VoiceResponse;
  timestamp: number;
  ttl: number;
}

interface BatchRequest {
  voice: VoiceArchetype;
  prompt: string;
  context: ProjectContext;
  resolve: (response: VoiceResponse) => void;
  reject: (error: Error) => void;
}

interface ModelSession {
  model: string;
  lastUsed: number;
  keepAlive: boolean;
  conversationId?: string;
}

export class OptimizedModelClient {
  private client: AxiosInstance;
  private config: LocalModelConfig;
  private responseCache = new Map<string, CacheEntry>();
  private batchQueue: BatchRequest[] = [];
  private batchTimer: NodeJS.Timeout | null = null;
  private activeSessions = new Map<string, ModelSession>();
  private connectionPool: AxiosInstance[] = [];
  private requestQueue: Array<() => Promise<any>> = [];
  private maxConcurrentRequests = 3;
  private activeRequests = 0;

  // Performance optimization settings
  private readonly BATCH_SIZE = 3;
  private readonly BATCH_TIMEOUT = 500; // ms
  private readonly CACHE_TTL = 5 * 60 * 1000; // 5 minutes
  private readonly SESSION_TIMEOUT = 10 * 60 * 1000; // 10 minutes
  private readonly MAX_RETRIES = 2;

  constructor(config: LocalModelConfig) {
    this.config = config;
    
    // Create optimized axios client with keep-alive
    this.client = axios.create({
      baseURL: config.endpoint,
      timeout: config.timeout,
      headers: {
        'Content-Type': 'application/json',
        'Connection': 'keep-alive',
      },
      // Enable connection pooling
      maxRedirects: 0,
      // Increase socket limits
      maxContentLength: 50 * 1024 * 1024, // 50MB
    });

    // Initialize connection pool
    this.initializeConnectionPool();
    
    // Set up cleanup intervals
    this.setupCleanupTimers();
    
    logger.info('ðŸš€ Optimized Model Client initialized', {
      endpoint: config.endpoint,
      model: config.model,
      batchSize: this.BATCH_SIZE,
      cacheEnabled: true
    });
  }

  /**
   * Generate voice response with optimizations
   */
  async generateVoiceResponse(
    voice: VoiceArchetype,
    prompt: string,
    context: ProjectContext = { files: [] }
  ): Promise<VoiceResponse> {
    // Check cache first
    const cacheKey = this.generateCacheKey(voice, prompt, context);
    const cached = this.getCachedResponse(cacheKey);
    if (cached) {
      logger.debug('ðŸ“¦ Using cached response');
      return cached;
    }

    // Add to batch queue for processing
    return new Promise((resolve, reject) => {
      this.batchQueue.push({ voice, prompt, context, resolve, reject });
      this.scheduleBatchProcessing();
    });
  }

  /**
   * Batch process requests for better throughput
   */
  private scheduleBatchProcessing(): void {
    if (this.batchTimer) {
      clearTimeout(this.batchTimer);
    }

    // Process immediately if batch is full
    if (this.batchQueue.length >= this.BATCH_SIZE) {
      this.processBatch();
      return;
    }

    // Otherwise wait for timeout
    this.batchTimer = setTimeout(() => {
      if (this.batchQueue.length > 0) {
        this.processBatch();
      }
    }, this.BATCH_TIMEOUT);
  }

  /**
   * Process a batch of requests
   */
  private async processBatch(): Promise<void> {
    if (this.batchQueue.length === 0) return;

    const batch = this.batchQueue.splice(0, this.BATCH_SIZE);
    logger.debug(`ðŸ”„ Processing batch of ${batch.length} requests`);

    // Process requests with controlled concurrency
    const promises = batch.map(request => 
      this.processRequest(request).catch(error => {
        request.reject(error);
      })
    );

    await Promise.all(promises);
  }

  /**
   * Process individual request with optimizations
   */
  private async processRequest(request: BatchRequest): Promise<void> {
    const { voice, prompt, context, resolve, reject } = request;

    try {
      // Queue request to control concurrency
      const response = await this.queueRequest(() => 
        this.executeRequest(voice, prompt, context)
      );

      // Cache the response
      const cacheKey = this.generateCacheKey(voice, prompt, context);
      this.setCachedResponse(cacheKey, response);

      resolve(response);
    } catch (error) {
      reject(error instanceof Error ? error : new Error('Unknown error'));
    }
  }

  /**
   * Execute request with session management and retries
   */
  private async executeRequest(
    voice: VoiceArchetype,
    prompt: string,
    context: ProjectContext
  ): Promise<VoiceResponse> {
    const modelName = await this.selectOptimalModel();
    const session = this.getOrCreateSession(modelName);

    // Build optimized request payload
    const payload = this.buildOptimizedPayload(voice, prompt, context, session);

    let lastError: Error | null = null;
    
    // Retry logic with exponential backoff
    for (let attempt = 1; attempt <= this.MAX_RETRIES; attempt++) {
      try {
        const startTime = Date.now();
        
        // Make the request with session context
        const response = await this.client.post('/api/generate', payload);
        
        const duration = Date.now() - startTime;
        logger.debug(`âœ… Request completed in ${duration}ms (attempt ${attempt})`);

        // Update session
        session.lastUsed = Date.now();

        // Parse and return response
        return this.parseResponse(response.data, voice.name, duration);

      } catch (error) {
        lastError = error instanceof Error ? error : new Error('Unknown error');
        logger.warn(`âš ï¸ Request attempt ${attempt} failed:`, lastError.message);

        // Exponential backoff
        if (attempt < this.MAX_RETRIES) {
          const delay = Math.pow(2, attempt - 1) * 1000; // 1s, 2s, 4s...
          await new Promise(resolve => setTimeout(resolve, delay));
        }

        // Clear session on repeated failures
        if (attempt === this.MAX_RETRIES) {
          this.activeSessions.delete(modelName);
        }
      }
    }

    throw lastError || new Error('All retry attempts failed');
  }

  /**
   * Build optimized request payload
   */
  private buildOptimizedPayload(
    voice: VoiceArchetype,
    prompt: string,
    context: ProjectContext,
    session: ModelSession
  ): any {
    // Optimize context to reduce token usage
    const optimizedContext = this.optimizeContext(context);
    
    // Build system prompt with context
    const systemPrompt = this.buildSystemPrompt(voice, optimizedContext);
    
    return {
      model: session.model,
      prompt: `${systemPrompt}\n\nUser: ${prompt}\n\nAssistant:`,
      stream: false,
      options: {
        temperature: voice.temperature,
        top_p: 0.9,
        top_k: 40,
        num_predict: this.config.maxTokens,
        // Keep context alive for session reuse
        num_ctx: Math.min(4096, this.config.maxTokens * 2),
        // Optimize for performance
        numa: false,
        num_thread: -1, // Use all available threads
        repeat_penalty: 1.1,
        // Enable keep-alive for session persistence
        keep_alive: '10m'
      },
      // Session context for continuation
      context: session.conversationId ? [session.conversationId] : undefined
    };
  }

  /**
   * Optimize context to reduce token usage while preserving important information
   */
  private optimizeContext(context: ProjectContext): ProjectContext {
    if (!context.files || context.files.length === 0) {
      return context;
    }

    // Limit number of files to most relevant
    const maxFiles = 3;
    const optimizedFiles = context.files
      .slice(0, maxFiles)
      .map(file => ({
        ...file,
        // Truncate large files to preserve context window
        content: file.content.length > 2000 
          ? file.content.substring(0, 2000) + '\n... (truncated)'
          : file.content
      }));

    return {
      ...context,
      files: optimizedFiles
    };
  }

  /**
   * Build system prompt with context
   */
  private buildSystemPrompt(voice: VoiceArchetype, context: ProjectContext): string {
    let systemPrompt = voice.systemPrompt;

    if (context.files && context.files.length > 0) {
      systemPrompt += '\n\nProject Context:\n';
      context.files.forEach((file, index) => {
        systemPrompt += `File ${index + 1}: ${file.path} (${file.language})\n`;
        systemPrompt += `\`\`\`${file.language}\n${file.content}\n\`\`\`\n\n`;
      });
    }

    if (context.projectType) {
      systemPrompt += `\nProject Type: ${context.projectType}`;
    }

    return systemPrompt;
  }

  /**
   * Parse API response into VoiceResponse format
   */
  private parseResponse(data: any, voiceName: string, duration: number): VoiceResponse {
    const content = data.response || data.message?.content || '';
    
    // Estimate confidence based on response characteristics
    const confidence = this.estimateConfidence(content, duration);
    
    // Estimate token usage
    const tokensUsed = Math.ceil((content.length + (data.prompt_eval_count || 0)) / 4);

    return {
      content,
      voice: voiceName,
      confidence,
      reasoning: `Generated in ${duration}ms using optimized client`,
      tokens_used: tokensUsed
    };
  }

  /**
   * Estimate confidence based on response quality indicators
   */
  private estimateConfidence(content: string, duration: number): number {
    let confidence = 0.7; // Base confidence

    // Content quality indicators
    if (content.length > 100) confidence += 0.1;
    if (content.includes('```')) confidence += 0.05; // Code blocks
    if (content.match(/\d+\./)) confidence += 0.05; // Numbered lists
    if (content.includes('**') || content.includes('##')) confidence += 0.05; // Formatting

    // Performance indicators
    if (duration < 10000) confidence += 0.1; // Fast response
    if (duration > 30000) confidence -= 0.1; // Slow response

    return Math.min(Math.max(confidence, 0), 1);
  }

  /**
   * Select optimal model based on current conditions
   */
  private async selectOptimalModel(): Promise<string> {
    // Use configured model if not auto
    if (this.config.model !== 'auto') {
      return this.config.model;
    }

    // Select based on system load and available models
    // For now, return a sensible default
    return 'llama3.2:latest';
  }

  /**
   * Get or create model session for context reuse
   */
  private getOrCreateSession(modelName: string): ModelSession {
    const existing = this.activeSessions.get(modelName);
    
    if (existing && (Date.now() - existing.lastUsed < this.SESSION_TIMEOUT)) {
      return existing;
    }

    const session: ModelSession = {
      model: modelName,
      lastUsed: Date.now(),
      keepAlive: true,
      conversationId: `session_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`
    };

    this.activeSessions.set(modelName, session);
    return session;
  }

  /**
   * Queue request to control concurrency
   */
  private async queueRequest<T>(requestFn: () => Promise<T>): Promise<T> {
    return new Promise((resolve, reject) => {
      const executeRequest = async () => {
        if (this.activeRequests >= this.maxConcurrentRequests) {
          // Re-queue if at capacity
          setTimeout(() => this.queueRequest(requestFn).then(resolve).catch(reject), 100);
          return;
        }

        this.activeRequests++;
        try {
          const result = await requestFn();
          resolve(result);
        } catch (error) {
          reject(error);
        } finally {
          this.activeRequests--;
          // Process next queued request
          const nextRequest = this.requestQueue.shift();
          if (nextRequest) {
            nextRequest();
          }
        }
      };

      executeRequest();
    });
  }

  /**
   * Generate cache key for response caching
   */
  private generateCacheKey(voice: VoiceArchetype, prompt: string, context: ProjectContext): string {
    const contextHash = JSON.stringify({
      files: context.files?.map(f => ({ path: f.path, length: f.content.length })),
      projectType: context.projectType
    });
    
    return `${voice.id}:${prompt.substring(0, 100)}:${contextHash}`;
  }

  /**
   * Get cached response if valid
   */
  private getCachedResponse(key: string): VoiceResponse | null {
    const entry = this.responseCache.get(key);
    
    if (!entry) return null;
    
    if (Date.now() - entry.timestamp > entry.ttl) {
      this.responseCache.delete(key);
      return null;
    }
    
    return entry.response;
  }

  /**
   * Cache response with TTL
   */
  private setCachedResponse(key: string, response: VoiceResponse): void {
    this.responseCache.set(key, {
      response,
      timestamp: Date.now(),
      ttl: this.CACHE_TTL
    });
  }

  /**
   * Initialize connection pool for better performance
   */
  private initializeConnectionPool(): void {
    // Create additional connections for parallel processing
    for (let i = 0; i < this.maxConcurrentRequests - 1; i++) {
      const client = axios.create({
        baseURL: this.config.endpoint,
        timeout: this.config.timeout,
        headers: {
          'Content-Type': 'application/json',
          'Connection': 'keep-alive',
        }
      });
      this.connectionPool.push(client);
    }
  }

  /**
   * Set up cleanup timers for cache and sessions
   */
  private setupCleanupTimers(): void {
    // Clean up expired cache entries every 5 minutes
    setInterval(() => {
      const now = Date.now();
      for (const [key, entry] of this.responseCache.entries()) {
        if (now - entry.timestamp > entry.ttl) {
          this.responseCache.delete(key);
        }
      }
    }, 5 * 60 * 1000);

    // Clean up expired sessions every 10 minutes
    setInterval(() => {
      const now = Date.now();
      for (const [model, session] of this.activeSessions.entries()) {
        if (now - session.lastUsed > this.SESSION_TIMEOUT) {
          this.activeSessions.delete(model);
        }
      }
    }, 10 * 60 * 1000);
  }

  /**
   * Get performance statistics
   */
  getPerformanceStats(): any {
    return {
      cacheSize: this.responseCache.size,
      activeSessions: this.activeSessions.size,
      activeRequests: this.activeRequests,
      queueSize: this.requestQueue.length,
      batchQueueSize: this.batchQueue.length
    };
  }

  /**
   * Clear all caches and reset sessions
   */
  clearOptimizations(): void {
    this.responseCache.clear();
    this.activeSessions.clear();
    this.requestQueue.length = 0;
    this.batchQueue.length = 0;
    logger.info('ðŸ§¹ Optimizations cleared');
  }
}